# ğŸ§‘â€ğŸ’» **Softmax Regression with Batch Gradient Descent and Early Stopping from scratch**

This project focuses on implementing softmax regression using batch gradient descent with early stopping. The model is designed to classify data into multiple classes, and the implementation uses only NumPy, without relying on Scikit-Learn. The goal is to understand and apply the concepts of gradient descent and softmax regression to solve a multi-class classification problem.

---

## ğŸ“ **Available Models and Techniques**

### ğŸŒ **Model**
- **Softmax Regression**: A machine learning model for multi-class classification using the softmax function to convert logits into class probabilities.

### ğŸ”§ **Training Technique**
- **Batch Gradient Descent**: Optimization method used to minimize the loss function by updating model parameters based on the entire training dataset.
- **Early Stopping**: A regularization technique used to prevent overfitting by stopping training when the validation loss starts increasing.

### ğŸ“Š **Data Preprocessing**
- **Normalization**: The input features are normalized to zero mean and unit variance to improve the training process.
- **One-Hot Encoding**: Categorical labels are encoded into a binary matrix for model training.

### ğŸ“ˆ **Evaluation**
- **Cross-Entropy Loss**: Loss function used to evaluate the model's performance.
- **Accuracy**: Evaluating model performance by comparing the predicted labels against the actual labels.
- **Visualization**: Contour plots are used to visualize decision boundaries.

<p align="center"> Made with â¤ï¸ for learning </p>
